{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kvsnoufal/MSPhi2-for-classification-LoRA/blob/main/trainPhi2Lora%26Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning Microsoft Phi 2 model for Sequence Classification on QLoRA using huggingface trainer (Bits and Bytes Quantization)"
      ],
      "metadata": {
        "id": "sBm91AodP93o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers peft datasets evaluate wandb einops bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-GDLDrQz9zz",
        "outputId": "9a16fae8-cccc-4881-945c-b6291dacb99f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.36.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.41.3.post2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.25.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.39.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LABELS = 2\n",
        "MODEL_NM = \"microsoft/phi-2\"\n",
        "MAX_LEN = 512\n",
        "LR = 2e-4\n",
        "OUTPUT_DIR = 'Phi2-Seq-classification-QLoRa'\n",
        "MAX_STEPS = 3000"
      ],
      "metadata": {
        "id": "gi1OvSXYz--k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUBmYVgWz5Oi",
        "outputId": "35ebb574-2d1f-4dad-b906-e37577fec3b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6090 entries, 0 to 6089\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        6090 non-null   int64 \n",
            " 1   keyword   6037 non-null   object\n",
            " 2   location  4064 non-null   object\n",
            " 3   text      6090 non-null   object\n",
            " 4   target    6090 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 238.0+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1523 entries, 0 to 1522\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        1523 non-null   int64 \n",
            " 1   keyword   1515 non-null   object\n",
            " 2   location  1016 non-null   object\n",
            " 3   text      1523 non-null   object\n",
            " 4   target    1523 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 59.6+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3263 entries, 0 to 3262\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        3263 non-null   int64 \n",
            " 1   keyword   3237 non-null   object\n",
            " 2   location  2158 non-null   object\n",
            " 3   text      3263 non-null   object\n",
            " 4   target    3263 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 127.6+ KB\n",
            "None\n",
            "0    3470\n",
            "1    2620\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# data loading\n",
        "from datasets import load_dataset,Dataset\n",
        "import pandas as pd\n",
        "dataset = load_dataset(\"mehdiiraqui/twitter_disaster\")\n",
        "# Split the dataset into training and validation datasets\n",
        "data = dataset['train'].train_test_split(train_size=0.8, seed=42)\n",
        "# Rename the default \"test\" split to \"validation\"\n",
        "data['val'] = data.pop(\"test\")\n",
        "# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\n",
        "data['test'] = dataset['test']\n",
        "# explore data\n",
        "data.items()\n",
        "print(data['train'].to_pandas().info())\n",
        "print(data['val'].to_pandas().info())\n",
        "print(data['test'].to_pandas().info())\n",
        "print(data['train'].to_pandas()['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPLQ9wpMz5Ol",
        "outputId": "6b603b22-d663-4ace-9e94-ba54e7b28c84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.1622137404580153, 0.877521613832853)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# setting weights for loss function- to address class imbalance\n",
        "pos_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[1])\n",
        "neg_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[0])\n",
        "pos_weights,neg_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIX_G-POz5Om"
      },
      "source": [
        "### Data prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NvmeEZnz5On",
        "outputId": "70253d8c-fb01-432d-8c53-0e7f76ba033e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NM,trust_remote_code=True,)\n",
        "# set pad token - to avoid error while training\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bl-MnMwz5Oo",
        "outputId": "a4178206-10b9-46d1-fe5b-9a1b741934a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [1820, 5290, 3252, 13, 3740, 1378, 83, 13, 1073, 14, 72, 39, 23, 8322, 89, 23, 76, 80, 18], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def preprocessing_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n",
        "# test\n",
        "preprocessing_function(data['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIgj8zstz5Oo",
        "outputId": "bc2097e9-9f86-4c30-b431-84bff64742bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': tensor(0),\n",
              " 'input_ids': tensor([1820, 5290, 3252,   13, 3740, 1378,   83,   13, 1073,   14,   72,   39,\n",
              "           23, 8322,   89,   23,   76,   80,   18]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# ref: https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n",
        "col_to_delete = ['id', 'keyword','location', 'text']\n",
        "# Apply the preprocessing function and remove the undesired columns\n",
        "tokenized_datasets = data.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
        "# Rename the target to label as for HugginFace standards\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"target\", \"label\")\n",
        "# Set to torch format\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "# test\n",
        "tokenized_datasets['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y8SxsYo6z5Op"
      },
      "outputs": [],
      "source": [
        "#  It takes a batch of examples and ensures that each sequence in the batch has the same length\n",
        "#  by padding the shorter ones. Ensures fixed-size input sequences\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XCL6oQgz5Op"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691,
          "referenced_widgets": [
            "6ac0c2a5278d49b5af0046ba129e6d46",
            "08e14edddca949819cedfda092ba3053",
            "e3fcca92651148e7b860813f9424b27b",
            "11aaf9323c4a4a0f823780aa41baecb9",
            "bc0f2984dc6c478aba7fae92fd071cc6",
            "f94276f9f4f54a4b9151c7ff09b2cc75",
            "50fbea6058944353b30bf01452bef173",
            "3c28fb032d4647fb8641b09c12ea242e",
            "858f541d09314afd9b78809327ff1373",
            "e36da540d9e448f19c355be9fc05b5d6",
            "7a2bc5543b7944c59d3833230a6190e1"
          ]
        },
        "id": "dDoSq9hGz5Oq",
        "outputId": "0ec0af52-b78d-40f1-d0d2-63599b45369d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ac0c2a5278d49b5af0046ba129e6d46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (transformer): PhiModel(\n",
              "    (embd): Embedding(\n",
              "      (wte): Embedding(51200, 2560)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (h): ModuleList(\n",
              "      (0-31): 32 x ParallelBlock(\n",
              "        (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (mixer): MHA(\n",
              "          (rotary_emb): RotaryEmbedding()\n",
              "          (Wqkv): Linear4bit(in_features=2560, out_features=7680, bias=True)\n",
              "          (out_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
              "          (inner_attn): SelfAttention(\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (inner_cross_attn): CrossAttention(\n",
              "            (drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (mlp): MLP(\n",
              "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
              "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): CausalLMHead(\n",
              "    (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "    (linear): Linear(in_features=2560, out_features=51200, bias=True)\n",
              "  )\n",
              "  (loss): CausalLMLoss(\n",
              "    (loss_fct): CrossEntropyLoss()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from torch import nn\n",
        "# ref: https://www.kaggle.com/code/archanghosh/quantized-mistral-7b-approach\n",
        "# ref: https://huggingface.co/microsoft/phi-2/discussions/19\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype='float16',\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "    )\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "\n",
        "# load model in half precision - not working for training\n",
        "# basemodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\n",
        "# loads model in full precision - works\n",
        "# basemodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cuda\", trust_remote_code=True)\n",
        "# loads model with quantization\n",
        "basemodel = AutoModelForCausalLM.from_pretrained(MODEL_NM, quantization_config=bnb_config, device_map=device_map, trust_remote_code=True)\n",
        "#Setting the Pretraining_tp to 1 ensures we are using the Linear Layers to the max computation possible\n",
        "basemodel.config.pretraining_tp = 1\n",
        "basemodel.config.pad_token_id = tokenizer.pad_token_id\n",
        "basemodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkucLtR4z5Or"
      },
      "source": [
        "Model has 3 main blocks:\n",
        "1. Transformer block - backbone model - output shape - batch x seq_len x 2560\n",
        "2. lm_head - output batch x seq_len x 51200(token size)\n",
        "3. loss - Loss fn\n",
        "\n",
        "To reconfigure model for sequence classification:\n",
        "We use an existing sequence classification wrapper and modify it as follows:\n",
        "1. use transformer block from above\n",
        "2. change lm head to classification head\n",
        "3. use custom loss function\n",
        "\n",
        "We are using PhiForSequenceClassification class source code : https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/models/phi/modeling_phi.py#L1165"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0iCQqRXQz5Os"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from transformers.utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    is_flash_attn_2_available,\n",
        "    is_flash_attn_greater_or_equal_2_10,\n",
        "    logging,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "class PhiPreTrainedModel(PreTrainedModel):\n",
        "    config_class = basemodel.config_class\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _skip_keys_device_placement = \"past_key_values\"\n",
        "    _supports_flash_attn_2 = True\n",
        "    _supports_cache_class = True\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.initializer_range\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "\n",
        "#custom class - modified from PhiForSequenceClassification\n",
        "class PhiForSequenceClassificationModified(PhiPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = NUM_LABELS#changed\n",
        "        self.model = basemodel.transformer#changed\n",
        "        self.score = nn.Linear(basemodel.config.hidden_size, NUM_LABELS, bias=False)#changed\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embd.wte#changed\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embd.wte = value#changed\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(\"PHI_INPUTS_DOCSTRING\")\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        model_outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "\n",
        "        )\n",
        "        hidden_states = model_outputs#changed\n",
        "        logits = self.score(hidden_states)\n",
        "        # print(logits)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            batch_size = input_ids.shape[0]\n",
        "        else:\n",
        "            batch_size = inputs_embeds.shape[0]\n",
        "\n",
        "        if self.config.pad_token_id is None and batch_size != 1:\n",
        "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
        "        if self.config.pad_token_id is None:\n",
        "            sequence_lengths = -1\n",
        "        else:\n",
        "            if input_ids is not None:\n",
        "                sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1).to(\n",
        "                    logits.device\n",
        "                )\n",
        "            else:\n",
        "                sequence_lengths = -1\n",
        "\n",
        "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = labels.to(logits.device)\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(pooled_logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(pooled_logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (pooled_logits,) + model_outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=pooled_logits,\n",
        "            past_key_values=None,\n",
        "            hidden_states=None,\n",
        "            attentions=None,\n",
        "        )#changed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Cjer4vEBz5Ot"
      },
      "outputs": [],
      "source": [
        "model = PhiForSequenceClassificationModified(basemodel.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDPpCtS5z5Ot"
      },
      "source": [
        "#### Setup PEFT- LoRa.\n",
        "ref. https://huggingface.co/microsoft/phi-2/discussions/19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OVrwoiNz5Ot",
        "outputId": "de1bb496-5eb8-40b3-95fa-a62c5b7d9624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 15,733,760 || all params: 2,664,294,400 || trainable%: 0.5905413455810289\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "peft_model = get_peft_model(model, LoraConfig(\n",
        "                            task_type=TaskType.SEQ_CLS,\n",
        "                            r=32,\n",
        "                            lora_alpha=16,\n",
        "                            target_modules=[\n",
        "                            'Wqkv',\n",
        "                            'out_proj'\n",
        "                            ],\n",
        "                            bias=\"none\",\n",
        "                            lora_dropout=0.05, # Conventional\n",
        "                        ))\n",
        "peft_model.print_trainable_parameters()\n",
        "peft_model = peft_model.to(device='cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p6HnVSbWz5Ou"
      },
      "outputs": [],
      "source": [
        "# evaluation metrics and loss function for trainer\n",
        "# https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral\n",
        "# https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import Trainer,TrainingArguments\n",
        "def compute_metrics(eval_pred):\n",
        "    # All metrics are already predefined in the HF `evaluate` package\n",
        "    precision_metric = evaluate.load(\"precision\")\n",
        "    recall_metric = evaluate.load(\"recall\")\n",
        "    f1_metric= evaluate.load(\"f1\")\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores.\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}\n",
        "\n",
        "\n",
        "class WeightedCELossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        # Get model's predictions\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # Compute custom loss\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "TVG1a4_Rz5Ou",
        "outputId": "fd4401bd-712f-4d79-bd33-5d85fcdf695e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvsnoufal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231219_094939-3pe35bly</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kvsnoufal/Phi2-Seq-classification-QLora/runs/3pe35bly' target=\"_blank\">eternal-disco-7</a></strong> to <a href='https://wandb.ai/kvsnoufal/Phi2-Seq-classification-QLora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/kvsnoufal/Phi2-Seq-classification-QLora' target=\"_blank\">https://wandb.ai/kvsnoufal/Phi2-Seq-classification-QLora</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/kvsnoufal/Phi2-Seq-classification-QLora/runs/3pe35bly' target=\"_blank\">https://wandb.ai/kvsnoufal/Phi2-Seq-classification-QLora/runs/3pe35bly</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",#for qlora\n",
        "    save_steps=100,\n",
        "    save_total_limit =2,\n",
        "    logging_steps=10,\n",
        "    learning_rate=LR,\n",
        "    fp16=True,#for qlora\n",
        "    # bf16=True,\n",
        "    max_grad_norm=.3,\n",
        "    max_steps=MAX_STEPS,\n",
        "    warmup_ratio=.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to='wandb',\n",
        "    evaluation_strategy=\"steps\",# Evaluate the model every specified number of steps\n",
        "    eval_steps=100,\n",
        ")\n",
        "import wandb\n",
        "wandb.init(project=OUTPUT_DIR)\n",
        "phi2_trainer = WeightedCELossTrainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets[\"val\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "from copy import deepcopy\n",
        "\n",
        "from transformers import TrainerCallback\n",
        "class CustomCallback(TrainerCallback):\n",
        "    def __init__(self, trainer) -> None:\n",
        "        super().__init__()\n",
        "        self._trainer = trainer\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        if control.should_evaluate:\n",
        "            control_copy = deepcopy(control)\n",
        "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
        "            return control_copy\n",
        "phi2_trainer.add_callback(CustomCallback(phi2_trainer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EZ9ctPsyz5Ov",
        "outputId": "214e2edb-ea28-4f9d-f8e0-1d5a22c74b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 1:50:54, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-score</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.392500</td>\n",
              "      <td>1.283911</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>0.387097</td>\n",
              "      <td>0.464516</td>\n",
              "      <td>0.618516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.253200</td>\n",
              "      <td>0.996144</td>\n",
              "      <td>0.685430</td>\n",
              "      <td>0.635945</td>\n",
              "      <td>0.659761</td>\n",
              "      <td>0.719632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.046600</td>\n",
              "      <td>2.312223</td>\n",
              "      <td>0.876106</td>\n",
              "      <td>0.304147</td>\n",
              "      <td>0.451539</td>\n",
              "      <td>0.684176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.412700</td>\n",
              "      <td>1.017044</td>\n",
              "      <td>0.765988</td>\n",
              "      <td>0.809524</td>\n",
              "      <td>0.787155</td>\n",
              "      <td>0.812869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.187800</td>\n",
              "      <td>1.219186</td>\n",
              "      <td>0.619870</td>\n",
              "      <td>0.881720</td>\n",
              "      <td>0.727964</td>\n",
              "      <td>0.718319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.192000</td>\n",
              "      <td>0.916777</td>\n",
              "      <td>0.692403</td>\n",
              "      <td>0.854071</td>\n",
              "      <td>0.764787</td>\n",
              "      <td>0.775443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.685500</td>\n",
              "      <td>1.089889</td>\n",
              "      <td>0.856884</td>\n",
              "      <td>0.726575</td>\n",
              "      <td>0.786367</td>\n",
              "      <td>0.831254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.906400</td>\n",
              "      <td>0.934007</td>\n",
              "      <td>0.771341</td>\n",
              "      <td>0.777266</td>\n",
              "      <td>0.774292</td>\n",
              "      <td>0.806303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.088500</td>\n",
              "      <td>1.229913</td>\n",
              "      <td>0.751037</td>\n",
              "      <td>0.834101</td>\n",
              "      <td>0.790393</td>\n",
              "      <td>0.810900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.881500</td>\n",
              "      <td>0.870570</td>\n",
              "      <td>0.829752</td>\n",
              "      <td>0.771121</td>\n",
              "      <td>0.799363</td>\n",
              "      <td>0.834537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.699700</td>\n",
              "      <td>0.922248</td>\n",
              "      <td>0.881890</td>\n",
              "      <td>0.688172</td>\n",
              "      <td>0.773080</td>\n",
              "      <td>0.827315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.053800</td>\n",
              "      <td>1.035809</td>\n",
              "      <td>0.825243</td>\n",
              "      <td>0.783410</td>\n",
              "      <td>0.803783</td>\n",
              "      <td>0.836507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>0.981395</td>\n",
              "      <td>0.904366</td>\n",
              "      <td>0.668203</td>\n",
              "      <td>0.768551</td>\n",
              "      <td>0.827971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.262800</td>\n",
              "      <td>0.868113</td>\n",
              "      <td>0.787009</td>\n",
              "      <td>0.800307</td>\n",
              "      <td>0.793602</td>\n",
              "      <td>0.822062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.855000</td>\n",
              "      <td>0.807290</td>\n",
              "      <td>0.830420</td>\n",
              "      <td>0.729647</td>\n",
              "      <td>0.776778</td>\n",
              "      <td>0.820749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.426500</td>\n",
              "      <td>1.244725</td>\n",
              "      <td>0.733871</td>\n",
              "      <td>0.838710</td>\n",
              "      <td>0.782796</td>\n",
              "      <td>0.801051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.465100</td>\n",
              "      <td>1.020271</td>\n",
              "      <td>0.777448</td>\n",
              "      <td>0.804916</td>\n",
              "      <td>0.790943</td>\n",
              "      <td>0.818122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.817800</td>\n",
              "      <td>1.200684</td>\n",
              "      <td>0.924242</td>\n",
              "      <td>0.655914</td>\n",
              "      <td>0.767296</td>\n",
              "      <td>0.829941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.088100</td>\n",
              "      <td>1.270816</td>\n",
              "      <td>0.918033</td>\n",
              "      <td>0.602151</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.806960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.168300</td>\n",
              "      <td>0.855105</td>\n",
              "      <td>0.695541</td>\n",
              "      <td>0.838710</td>\n",
              "      <td>0.760446</td>\n",
              "      <td>0.774130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.850900</td>\n",
              "      <td>0.889545</td>\n",
              "      <td>0.889524</td>\n",
              "      <td>0.717358</td>\n",
              "      <td>0.794218</td>\n",
              "      <td>0.841103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.717500</td>\n",
              "      <td>1.046346</td>\n",
              "      <td>0.906504</td>\n",
              "      <td>0.685100</td>\n",
              "      <td>0.780402</td>\n",
              "      <td>0.835194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.243600</td>\n",
              "      <td>0.902307</td>\n",
              "      <td>0.822764</td>\n",
              "      <td>0.777266</td>\n",
              "      <td>0.799368</td>\n",
              "      <td>0.833224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.918400</td>\n",
              "      <td>0.977299</td>\n",
              "      <td>0.768012</td>\n",
              "      <td>0.818740</td>\n",
              "      <td>0.792565</td>\n",
              "      <td>0.816809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.124200</td>\n",
              "      <td>1.004722</td>\n",
              "      <td>0.881801</td>\n",
              "      <td>0.721966</td>\n",
              "      <td>0.793919</td>\n",
              "      <td>0.839790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.473000</td>\n",
              "      <td>1.155694</td>\n",
              "      <td>0.774003</td>\n",
              "      <td>0.804916</td>\n",
              "      <td>0.789157</td>\n",
              "      <td>0.816152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.185600</td>\n",
              "      <td>0.968330</td>\n",
              "      <td>0.767204</td>\n",
              "      <td>0.804916</td>\n",
              "      <td>0.785607</td>\n",
              "      <td>0.812213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.903700</td>\n",
              "      <td>0.916902</td>\n",
              "      <td>0.779940</td>\n",
              "      <td>0.800307</td>\n",
              "      <td>0.789992</td>\n",
              "      <td>0.818122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.861200</td>\n",
              "      <td>0.838760</td>\n",
              "      <td>0.766467</td>\n",
              "      <td>0.786482</td>\n",
              "      <td>0.776346</td>\n",
              "      <td>0.806303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.115800</td>\n",
              "      <td>0.975317</td>\n",
              "      <td>0.854867</td>\n",
              "      <td>0.741935</td>\n",
              "      <td>0.794408</td>\n",
              "      <td>0.835850</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory Phi2-Seq-classification-QLoRa/checkpoint-100 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory Phi2-Seq-classification-QLoRa/checkpoint-200 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3000, training_loss=0.9764829754432043, metrics={'train_runtime': 6659.8268, 'train_samples_per_second': 1.802, 'train_steps_per_second': 0.45, 'total_flos': 2829283971379200.0, 'train_loss': 0.9764829754432043, 'epoch': 1.97})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# train\n",
        "phi2_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "best_step = 3000\n",
        "best_model_ckpt = os.path.join(OUTPUT_DIR,f'checkpoint-{best_step}')\n",
        "\n",
        "from peft import PeftModel\n",
        "loaded_model = PeftModel.from_pretrained(model,best_model_ckpt,is_trainable=False)\n",
        "# phi2_trainer.model = loaded_model\n",
        "phi2_trainer = WeightedCELossTrainer(\n",
        "    model=loaded_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets[\"val\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "HBM-WuFATfNt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi2_trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "5rrydi38TfCr",
        "outputId": "eb2af214-435d-4f76-9424-b5f7e63d5303"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='191' max='191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [191/191 01:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.9753166437149048,\n",
              " 'eval_precision': 0.8548672566371681,\n",
              " 'eval_recall': 0.7419354838709677,\n",
              " 'eval_f1-score': 0.794407894736842,\n",
              " 'eval_accuracy': 0.8358502954694682,\n",
              " 'eval_runtime': 71.1432,\n",
              " 'eval_samples_per_second': 21.408,\n",
              " 'eval_steps_per_second': 2.685}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = phi2_trainer.predict(tokenized_datasets['test'])\n",
        "pred_probas = torch.nn.functional.softmax(torch.tensor(preds.predictions),dim=1)\n",
        "pred_probas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "EDjAADQQcU5I",
        "outputId": "5cb9ccf5-7052-4308-ee4e-7ee1f7cf860a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4.6733e-01, 5.3267e-01],\n",
              "        [1.9411e-04, 9.9981e-01],\n",
              "        [2.4826e-04, 9.9975e-01],\n",
              "        ...,\n",
              "        [4.4520e-05, 9.9996e-01],\n",
              "        [1.4466e-03, 9.9855e-01],\n",
              "        [4.6839e-05, 9.9995e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data['test'].to_pandas()['target'].value_counts()\n",
        "pred_class  = pred_probas.argmax(1)\n",
        "pred_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rvbydQKcXNk",
        "outputId": "41596c41-400b-45b3-dd87-c9933c3d0894"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1,  ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(pred_class.cpu().numpy()).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxZIzQT2cdFS",
        "outputId": "3b288625-04fd-4cf4-aa0f-741beb18a4b1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2088\n",
              "1    1175\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCckPTeOcfuq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ac0c2a5278d49b5af0046ba129e6d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08e14edddca949819cedfda092ba3053",
              "IPY_MODEL_e3fcca92651148e7b860813f9424b27b",
              "IPY_MODEL_11aaf9323c4a4a0f823780aa41baecb9"
            ],
            "layout": "IPY_MODEL_bc0f2984dc6c478aba7fae92fd071cc6"
          }
        },
        "08e14edddca949819cedfda092ba3053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f94276f9f4f54a4b9151c7ff09b2cc75",
            "placeholder": "​",
            "style": "IPY_MODEL_50fbea6058944353b30bf01452bef173",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e3fcca92651148e7b860813f9424b27b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c28fb032d4647fb8641b09c12ea242e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_858f541d09314afd9b78809327ff1373",
            "value": 2
          }
        },
        "11aaf9323c4a4a0f823780aa41baecb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e36da540d9e448f19c355be9fc05b5d6",
            "placeholder": "​",
            "style": "IPY_MODEL_7a2bc5543b7944c59d3833230a6190e1",
            "value": " 2/2 [00:27&lt;00:00, 11.72s/it]"
          }
        },
        "bc0f2984dc6c478aba7fae92fd071cc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f94276f9f4f54a4b9151c7ff09b2cc75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50fbea6058944353b30bf01452bef173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c28fb032d4647fb8641b09c12ea242e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "858f541d09314afd9b78809327ff1373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e36da540d9e448f19c355be9fc05b5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2bc5543b7944c59d3833230a6190e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}